
Now I'll create the **comprehensive Quest Mode prompt** that ties everything together:

---

# ðŸŽ¯ **QUEST MODE MASTER IMPLEMENTATION PROMPT**

## **Project Goal**
Build a **Production-Grade Multi-Agent Orchestration System** with **CAI (Cybersecurity AI) Framework Integration**, featuring:
- **Nemotron-8B** as orchestrator (planning, decomposition, routing)
- **Qwen3:1.7b** for technical/coding tasks
- **DictaLM-1.7B** for Hebrew linguistics & documentation
- **Dynamic model discovery, capability assessment, and auto-swapping**
- **CAI agentic patterns**: Agents, Tools, Handoffs, Patterns, Guardrails, HITL
- **LangGraph** state machine for workflow orchestration
- **Docker sandbox** for safe code execution
- **ChromaDB** vector memory + SQLite structured storage
- **MCP (Model Context Protocol)** integration (2000+ tools)
- **Real-time streaming** with agent messaging bus

---

## **IMPLEMENTATION PHASES** (Execute in Order)

### **PHASE 1: Core Infrastructure [P0 - Critical Foundation]**

**Goal:** Establish the foundational systems that all other components depend on.

#### **Task 1.1: Dynamic Model Management System**
**Files to create:**
- `src/core/model_discovery.py` - Auto-detect Ollama models, assess capabilities
- `src/core/model_selector.py` - Dynamic model selection based on task requirements
- `src/core/model_watcher.py` - Background thread watching for new models
- `src/core/agent_registry.py` - Enhanced agent registry with model metadata

**Implementation requirements:**
1. Use **Ollama Python API** to list installed models
2. Benchmark each model using **3 tests**: coding (prime number function), reasoning (logic puzzle), speed (tokens/sec)
3. Store capabilities in SQLite: `agent_registry.db` with schema:
   ```sql
   CREATE TABLE agents (
     id TEXT PRIMARY KEY,
     model_name TEXT,
     role TEXT,
     capabilities TEXT,  -- JSON: {coding: 85, reasoning: 72, speed: 45}
     quality_score REAL,
     vram_required REAL,
     last_updated TIMESTAMP
   );
   ```
4. Start background watcher on orchestrator initialization (scan every 60s)
5. Add CLI command: `python run_orchestrator.py --scan-models`

**Reference CAI files:**
- `cai/src/cai/agents/factory.py` - Agent factory pattern
- `cai/src/cai/agents/patterns/` - Pattern implementations

---

#### **Task 1.2: Agent Model Abstraction (CAI-Style)**
**Files to create:**
- `src/core/agent_model.py` - Agent dataclass mirroring CAI's Agent properties

**Implementation requirements:**
1. Define `AgentDefinition` dataclass with:
   - `id`, `name`, `description`, `instructions` (string or callable)
   - `model_name`, `tools` (list of tool IDs), `handoffs` (list of agent IDs)
   - `input_guardrails`, `output_guardrails`, `pattern` (optional)
2. Add `clone()` method for creating agent instances with different models
3. Support **dynamic instructions** via callable: `instructions(context, agent) -> str`
4. Integrate with existing `AgentRegistry`

**Reference CAI files:**
- `cai/docs/agents.md` - Agent properties and structure
- `cai/src/cai/agents/web_pentester.py` - Example agent definition

---

#### **Task 1.3: Agent Factory & Discovery**
**Files to create:**
- `src/core/agent_factory.py` - Dynamic agent factory with auto-discovery

**Implementation requirements:**
1. Implement `discover_agent_factories()` - Scan `src/agents/` directory
2. For each `*_agent.py` file, detect top-level `agent` variable
3. Create factory function: `create_agent_instance(agent_id, model_override, custom_name)`
4. Support **parallel agents** via agent_id prefix (e.g., `P1_coder`, `P2_coder`)
5. Auto-inject MCP tools if configured for agent
6. Add to `AgentRegistry`: `get_agent_factory(agent_id)`, `create_agent_instance(...)`

**Reference CAI files:**
- `cai/src/cai/agents/factory.py` - Lines 1-199 (complete factory implementation)

---

### **PHASE 2: CAI Agentic Patterns & Handoffs [P0 - Core Patterns]**

**Goal:** Implement the formal agentic pattern system: **AP = (A, H, D, C, E)**

#### **Task 2.1: Agentic Patterns Module**
**Files to create:**
- `src/patterns/cai_patterns.py` - Pattern definitions and executor

**Implementation requirements:**
1. Define `PatternType` enum: `SWARM`, `HIERARCHICAL`, `CHAIN`, `AUCTION`, `RECURSIVE`, `PARALLEL`
2. Implement `AgenticPattern` dataclass:
   ```python
   @dataclass
   class AgenticPattern:
       name: str
       pattern_type: PatternType
       agents: List[str]  # Agent IDs
       handoff_rules: Dict[str, str]  # from_agent -> next_agent
       entry_agent: str
       termination_condition: Callable[[Dict], bool]
   ```
3. Predefine patterns:
   - `ctf_swarm` - Decentralized CTF team (recon â†’ vuln_scan â†’ exploit â†’ report)
   - `security_pipeline` - Linear security assessment chain
   - `code_review_recursive` - Agent refines code until tests pass
   - `parallel_analysis` - Multiple agents analyze different aspects simultaneously

**Reference CAI files:**
- `cai/docs/cai_architecture.md` - Lines 100-130 (formal pattern definition)
- `cai/src/cai/agents/patterns/` - Pattern implementations

---

#### **Task 2.2: Handoff Manager (Agent Delegation)**
**Files to create:**
- `src/patterns/handoffs.py` - CAI-style handoff system

**Implementation requirements:**
1. Implement `HandoffManager` with:
   - `handoff(from_agent, to_agent, context, input_filter=None)`
   - `auto_handoff(current_agent, result, pattern)` - Uses pattern's handoff_rules
2. Support **handoff callbacks**: `on_handoff(ctx, input_data)` executed before transfer
3. Implement **input filters**: remove tools, filter messages, transform context
4. Log all handoffs to message bus and structured DB
5. Integrate with `AgentMessageBus` (next task)

**Reference CAI files:**
- `cai/docs/handoffs.md` - Complete handoff documentation
- `cai/docs/handoffs.md` Lines 139-234 - Full example with callbacks

---

#### **Task 2.3: Agent Message Bus**
**Files to create:**
- `src/orchestration/agent_messaging.py` - Inter-agent communication

**Implementation requirements:**
1. Implement pub/sub message bus:
   - `publish(topic, message, sender_id)`
   - `subscribe(topic, callback)`
   - Topics: `agent.started`, `agent.completed`, `handoff.initiated`, `tool.executed`
2. Store messages in SQLite: `agent_messages` table
3. Provide query interface: `get_messages(agent_id, topic, since)`
4. Stream messages via SSE for real-time monitoring

**Reference CAI architecture:**
- Communication Protocol: **C: A Ã— A â†’ M** (message function between agents)

---

### **PHASE 3: LangGraph Workflow Engine [P0 - Execution Layer]**

**Goal:** Implement state machine orchestration with LangGraph

#### **Task 3.1: LangGraph Multi-Agent Workflow**
**Files to create:**
- `src/orchestration/langgraph_workflow.py` - LangGraph state machine

**Implementation requirements:**
1. Define `WorkflowState`:
   ```python
   class WorkflowState(TypedDict):
       task_id: str
       description: str
       pattern: str
       current_agent: str
       context: Dict
       result: Optional[str]
       history: List[Dict]
       checkpoint_id: Optional[str]
   ```
2. Implement `MultiAgentWorkflow`:
   - `create_graph(tasks, pattern)` - Build StateGraph from task DAG
   - Node function: Select agent â†’ Run with guardrails â†’ Check handoff â†’ Store result
   - Edge function: Check if handoff needed, route to next agent or END
3. Use **checkpointing**: Save state after each task to `data/checkpoints.db`
4. Support **parallel execution**: Multiple agents run simultaneously for independent tasks

**Reference architecture:**
- See `MVP_ROADMAP.md` Part F1 (LangGraph Workflow Engine)

---

#### **Task 3.2: Orchestrator Integration with LangGraph**
**Files to modify:**
- `src/orchestration/nemotron_orchestrator.py` - Integrate LangGraph

**Implementation requirements:**
1. Add `MultiAgentWorkflow` to orchestrator
2. Modify `decompose_and_assign()` to output:
   - Task DAG with dependencies
   - Pattern selection for each task group
   - Agent assignments
3. Execute via: `workflow.execute_graph(tasks, pattern, context)`
4. Stream progress updates via message bus
5. Support **continue mode**: Resume from checkpoint after human approval

**Reference CAI concepts:**
- **Turns**: Cycle of interactions finishing when agent returns None
- **HITL**: Human-in-the-loop via Ctrl+C (see Task 6.3)

---

### **PHASE 4: Security Agents & Tools [P0 - CAI Security Layer]**

**Goal:** Implement CAI-inspired security agents with tools and guardrails

#### **Task 4.1: Define Security Agents**
**Files to create:**
- `src/agents/web_pentester_agent.py` - Web application security testing
- `src/agents/retester_agent.py` - Vulnerability revalidation
- `src/agents/report_agent.py` - Professional security reports

**Implementation requirements:**
1. **web_pentester_agent**:
   - Model: `qwen3:1.7b` (fast, good at structured tasks)
   - Tools: `bash_exec`, `filesystem_read/write`, `nmap_scan`, `curl_probe`
   - Prompt: Adapt `cai/src/cai/prompts/system_web_pentester.md`
   - Handoffs: Can delegate to `retester_agent`, `report_agent`
   - Guardrails: Input/output security checks

2. **retester_agent**:
   - Model: `qwen3:1.7b`
   - Tools: `bash_exec`, `execute_code`, `web_fetch`
   - Focus: Re-validate vulnerabilities, eliminate false positives
   - Prompt: Triage agent prompt

3. **report_agent**:
   - Model: `dicta-il/dictalm2.0-instruct:1.7b` (excellent at Hebrew + documentation)
   - Tools: `filesystem_write`, `template_render`
   - Output: Professional security reports in Markdown/PDF

**Reference CAI files:**
- `cai/src/cai/agents/web_pentester.py` - Web pentester implementation
- `cai/src/cai/agents/retester.py` - Retester implementation
- `cai/src/cai/prompts/system_web_pentester.md` - System prompt

---

#### **Task 4.2: Security Tools Implementation**
**Files to create:**
- `src/tools/security_tools.py` - CAI-inspired security tools

**Implementation requirements:**
1. Implement tools following **CAI kill-chain categories**:
   - **Reconnaissance**: `nmap_scan`, `enumerate_linux`, `web_scrape`
   - **Exploitation**: `curl_probe`, `execute_exploit`, `check_default_creds`
   - **Escalation**: `privesc_check`, `sudo_test`
   - **Exfiltration**: `data_exfil` (for testing only, with guardrails)

2. All tools execute in **Docker sandbox** (next phase)
3. Register in `ToolRegistry` with categories
4. Add timeout limits and output size limits

**Reference CAI files:**
- `cai/docs/tools.md` - Tool architecture
- `cai/src/cai/tools/` - Tool implementations

---

#### **Task 4.3: Guardrails System (4-Layer Defense)**
**Files to create:**
- `src/patterns/guardrails.py` - Input/output guardrails

**Implementation requirements:**
1. **Input Guardrails** (`input_guardrail` decorator):
   - Check for prompt injection patterns
   - Validate scope (only attack allowed targets from `config/security_scope.yaml`)
   - Return `GuardrailFunctionOutput(tripwire_triggered, output_info)`

2. **Output Guardrails** (`output_guardrail` decorator):
   - Check for dangerous commands: `rm -rf`, fork bombs, `dd if=`, reverse shells
   - Validate target IPs against scope
   - Check for base64-encoded malicious payloads

3. **Guardrail Agent** (uses cheap/fast model):
   - Run guardrail check in parallel with main agent
   - If tripwire triggered, raise exception and halt execution

4. Integration with agents:
   - Add `input_guardrails`, `output_guardrails` to agent definitions
   - Example: `web_pentester_agent.input_guardrails = [scope_guardrail, injection_guardrail]`

**Reference CAI files:**
- `cai/docs/guardrails.md` - Complete guardrail documentation
- `cai/docs/cai_prompt_injection.md` - Prompt injection defense

---

### **PHASE 5: Memory & Context Systems [P1 - Persistence Layer]**

**Goal:** Implement vector memory, structured storage, and semantic search

#### **Task 5.1: Vector Memory Store (ChromaDB)**
**Files to create:**
- `src/memory/vector_store.py` - Semantic search with embeddings

**Implementation requirements:**
1. Use **ChromaDB** with persistent storage: `data/chromadb/`
2. Collections:
   - `task_outputs` - Agent outputs
   - `tool_executions` - Tool results
   - `agent_decisions` - Orchestrator decisions
   - `code_artifacts` - Generated code

3. API:
   - `store(text, metadata, collection)` - Store with auto-embedding
   - `query(query_text, collection, n_results=5)` - Semantic search
   - `get_context(task_id)` - Retrieve all related memories

4. Use **sentence-transformers** for embeddings (`all-MiniLM-L6-v2`)

**Reference:**
- See `MVP_ROADMAP.md` Part F3 (Vector Memory Store)

---

#### **Task 5.2: Structured Database Schema**
**Files to create:**
- `data/schema.sql` - Complete database schema
- `src/memory/structured_store.py` - SQLite interface

**Implementation requirements:**
1. Tables:
   ```sql
   CREATE TABLE task_executions (
     task_id TEXT PRIMARY KEY,
     description TEXT,
     agent_id TEXT,
     status TEXT,
     result TEXT,
     started_at TIMESTAMP,
     completed_at TIMESTAMP
   );
   
   CREATE TABLE agent_messages (
     id INTEGER PRIMARY KEY AUTOINCREMENT,
     topic TEXT,
     sender_id TEXT,
     content TEXT,
     timestamp TIMESTAMP
   );
   
   CREATE TABLE human_decisions (
     id INTEGER PRIMARY KEY AUTOINCREMENT,
     task_id TEXT,
     decision TEXT,  -- approve/reject/modify
     reason TEXT,
     timestamp TIMESTAMP
   );
   
   CREATE TABLE workflow_checkpoints (
     checkpoint_id TEXT PRIMARY KEY,
     state BLOB,
     created_at TIMESTAMP
   );
   ```

2. API:
   - `log_task(task_id, agent_id, description)`
   - `update_task_status(task_id, status, result)`
   - `query_tasks(filters)` - SQL query builder

---

#### **Task 5.3: Task Memory (Inter-Task Context)**
**Files to create:**
- `src/memory/task_memory.py` - In-memory context store

**Implementation requirements:**
1. Thread-safe in-memory store for current workflow
2. API:
   - `store_output(task_id, output)`
   - `get_input_context(task_id)` - Get outputs from dependency tasks
   - `clear()` - Reset between workflows

3. Automatically populate context for dependent tasks in LangGraph

---

### **PHASE 6: Docker Sandbox & Isolation [P0 - Security Layer]**

**Goal:** Safe code execution in isolated Docker container

#### **Task 6.1: Docker Sandbox Executor**
**Files to create:**
- `src/sandbox/docker_executor.py` - Docker container manager
- `docker/Dockerfile.sandbox` - Sandbox image definition

**Implementation requirements:**
1. **Dockerfile.sandbox**:
   ```dockerfile
   FROM python:3.11-slim
   RUN apt-get update && apt-get install -y \
       nmap curl netcat-openbsd wget git \
       && rm -rf /var/lib/apt/lists/*
   RUN pip install pytest requests
   WORKDIR /workspace
   ```

2. **DockerExecutor**:
   - `execute_code(code, language, timeout=30)`
   - `execute_command(cmd, timeout=30)`
   - Mount `/workspace` with read-only host bind
   - Network: isolated (no internet by default, enable with flag)
   - Resource limits: CPU 1.0, RAM 512MB

3. Integration with `BuiltinTools`:
   - All `requires_sandbox=True` tools use DockerExecutor
   - Stream stdout/stderr in real-time

**Reference:**
- See `MVP_ROADMAP.md` Part F (Docker Sandbox)

---

### **PHASE 7: External Integrations [P1 - Extended Capabilities]**

**Goal:** Integrate external frameworks and tools

#### **Task 7.1: MCP (Model Context Protocol) Integration**
**Files to create:**
- `src/integrations/mcp_client.py` - MCP protocol client

**Implementation requirements:**
1. Support MCP server connections via:
   - **SSE** (Server-Sent Events)
   - **stdio** (standard input/output)

2. Auto-discover MCP servers from `config/mcp_servers.yaml`:
   ```yaml
   servers:
     - name: filesystem
       command: npx
       args: ["-y", "@modelcontextprotocol/server-filesystem", "/workspace"]
     - name: git
       command: uvx
       args: ["mcp-server-git", "--repository", "/workspace"]
   ```

3. Expose MCP tools to agents via `ToolRegistry`
4. CLI: `python run_orchestrator.py --list-mcp-tools`

**Reference CAI files:**
- `cai/docs/mcp.md` - MCP documentation
- `cai/examples/mcp/` - MCP examples

---

#### **Task 7.2: Prompt Template System**
**Files to create:**
- `src/prompts/orchestrator_templates.py` - Nemotron prompts
- `src/prompts/agent_templates.py` - Agent system prompts

**Implementation requirements:**
1. Port CAI prompts to your system:
   - `DECOMPOSITION_PROMPT` - Task breakdown for Nemotron
   - `SYNTHESIS_PROMPT` - Output combination
   - `SECURITY_WEB_PENTEST_PROMPT` - Web pentester instructions

2. Support **dynamic prompts** with template variables:
   ```python
   DECOMPOSITION_PROMPT = """
   You are Nemotron orchestrator. Available agents:
   {agent_list}
   
   Available patterns:
   {pattern_list}
   
   User goal: {user_goal}
   
   Output task DAG with pattern selection.
   """
   ```

**Reference CAI files:**
- `cai/src/cai/prompts/` - All CAI system prompts
- `cai/docs/agents.md` Lines 332-347 - Dynamic instructions

---

### **PHASE 8: Monitoring & HITL [P2 - Operations Layer]**

**Goal:** Telemetry, tracing, and human-in-the-loop checkpoints

#### **Task 8.1: Execution Monitor**
**Files to create:**
- `src/monitoring/telemetry.py` - Real-time monitoring

**Implementation requirements:**
1. Log to `logs/execution.jsonl` (structured JSON logs):
   ```json
   {"event": "task.started", "task_id": "T1", "agent": "coder_qwen", "timestamp": "..."}
   {"event": "tool.executed", "tool_id": "bash_exec", "duration_ms": 234, "success": true}
   ```

2. CLI dashboard: `python run_orchestrator.py --dashboard`
   - Show: Tasks in progress, completed tasks, agent utilization
   - Update in real-time via SSE

3. Metrics:
   - Task success rate
   - Average task duration
   - Model performance by task type

---

#### **Task 8.2: Human Checkpoints (HITL)**
**Files to create:**
- `src/orchestration/human_checkpoint.py` - Approval workflow

**Implementation requirements:**
1. Checkpoint types:
   - **CRITICAL**: High-risk security actions (exploitation, data exfil)
   - **SYNTHESIS**: Before final output combination
   - **ERROR**: After retry exhaustion

2. Workflow:
   - Pause execution
   - Display: Task description, agent output, risk assessment
   - Prompt: `[A]pprove / [R]eject / [M]odify / [S]kip`
   - If rejected, log reason and halt workflow

3. Integration with LangGraph:
   - Add checkpoint node in graph before critical tasks

**Reference CAI:**
- `cai/docs/cai_architecture.md` Lines 160-191 - HITL documentation

---

#### **Task 8.3: CLI Enhancement**
**Files to modify:**
- `run_orchestrator.py` - Add rich CLI commands

**Implementation requirements:**
1. Commands:
   ```bash
   python run_orchestrator.py --goal "Build a web scraper"
   python run_orchestrator.py --scan-models
   python run_orchestrator.py --list-agents
   python run_orchestrator.py --list-tools
   python run_orchestrator.py --status
   python run_orchestrator.py --dashboard
   python run_orchestrator.py --query "What did web_pentester find?"
   python run_orchestrator.py --dry-run --goal "..."
   python run_orchestrator.py --continue-from CHECKPOINT_ID
   ```

2. Use `rich` library for formatted output:
   - Progress bars
   - Syntax highlighting
   - Tables

---

### **PHASE 9: Advanced Features [P2 - Polish]**

**Goal:** Dynamic tool creation, intent analysis, error recovery

#### **Task 9.1: Dynamic Tool Creation (DeepAgents Pattern)**
**Files to create:**
- `src/tools/dynamic_tools.py` - Agent-created tools

**Implementation requirements:**
1. Agents can create new tools by:
   - Writing Python function to `agent_tools/<tool_name>.py`
   - Registering in `ToolRegistry` with metadata

2. Example workflow:
   - Agent: "I need a tool to parse CSV files"
   - Orchestrator: Creates `agent_tools/parse_csv.py`
   - Validates code in sandbox
   - Registers tool
   - Agent uses new tool

3. Safety:
   - All dynamic tools sandboxed
   - Human approval for tool creation
   - Code review by guardrails

---

#### **Task 9.2: Deep Intent Analyzer**
**Files to create:**
- `src/orchestration/intent_analyzer.py` - NLU for user goals

**Implementation requirements:**
1. Use Nemotron to analyze user goal:
   - Extract: Task type, complexity, constraints
   - Classify: Security assessment vs coding vs documentation
   - Suggest: Pattern and agent assignment

2. Example:
   ```python
   analyze_intent("Build a secure REST API with authentication")
   # Output:
   {
     "task_type": "coding",
     "complexity": "high",
     "requires": ["security_review", "testing"],
     "suggested_pattern": "chain",
     "suggested_agents": ["coder_qwen", "web_pentester", "retester"],
     "estimated_duration": "30 minutes"
   }
   ```

---

#### **Task 9.3: Error Recovery & Retry Logic**
**Files to create:**
- `src/orchestration/error_recovery.py` - Retry strategies

**Implementation requirements:**
1. Retry strategies:
   - **Exponential backoff**: For transient failures (API timeouts)
   - **Agent swap**: Try different model if current fails
   - **Prompt refinement**: Add context from failure and retry

2. Max retries: 3
3. After exhaustion: Human checkpoint for decision

---

## **CRITICAL SUCCESS CRITERIA**

Before marking the implementation complete, verify ALL of these:

### **Functional Requirements**
- âœ… Orchestrator decomposes user goals into task DAGs
- âœ… Dynamic model discovery auto-detects new Ollama models
- âœ… Model selector chooses best model for each task type
- âœ… LangGraph executes task DAGs with state management
- âœ… Agent handoffs work between security agents (web_pentester â†’ retester â†’ report)
- âœ… Guardrails block dangerous commands and out-of-scope targets
- âœ… Docker sandbox isolates code execution
- âœ… Vector memory stores outputs with semantic search
- âœ… Structured DB logs all executions
- âœ… Human checkpoints pause before critical actions
- âœ… CLI provides all operations (scan, list, status, query, dashboard)
- âœ… Real-time streaming of agent outputs

### **CAI Integration Requirements**
- âœ… Agent definitions match CAI structure (name, description, instructions, tools, handoffs, guardrails)
- âœ… Agentic patterns formalized: AP = (A, H, D, C, E)
- âœ… Handoff manager implements CAI handoff protocol
- âœ… At least 3 security agents defined (web_pentester, retester, report)
- âœ… Security tools follow CAI kill-chain categories
- âœ… Guardrails use 4-layer defense from CAI research

### **Quality Requirements**
- âœ… All code runs without errors
- âœ… Complete docstrings for all classes/functions
- âœ… Type hints on all function signatures
- âœ… Unit tests for critical components (model selector, guardrails, handoffs)
- âœ… Example workflows in `examples/` directory:
  - `examples/security_assessment.py` - Full CTF-style workflow
  - `examples/code_generation.py` - Coding task with testing
  - `examples/dynamic_models.py` - Model discovery demo

---

## **EXECUTION STRATEGY FOR QUEST MODE**

### **Priority Order (P0 â†’ P1 â†’ P2)**
1. **P0 (Must-have before any testing):**
   - Phase 1: Core Infrastructure
   - Phase 2: CAI Patterns & Handoffs
   - Phase 3: LangGraph Workflow
   - Phase 4: Security Agents & Tools
   - Phase 6: Docker Sandbox

2. **P1 (Essential for production):**
   - Phase 5: Memory Systems
   - Phase 7: External Integrations
   - Phase 8: Monitoring & HITL

3. **P2 (Polish & advanced features):**
   - Phase 9: Dynamic Tools, Intent Analyzer, Error Recovery

### **Validation After Each Phase**
After completing each phase, run:
```bash
python -m pytest tests/test_phase{N}.py
python examples/phase{N}_demo.py
```

### **Final Integration Test**
Create `examples/full_security_workflow.py`:
```python
"""
Full security assessment workflow demonstrating all features:
1. Orchestrator decomposes "Assess web app security for http://target.com"
2. Model selector chooses agents dynamically
3. web_pentester scans target (with guardrails)
4. Handoff to retester for validation
5. Handoff to report_agent for documentation
6. Human checkpoint before final report
7. Vector memory stores all findings
8. CLI dashboard shows real-time progress
"""
```

---

## **REFERENCE DOCUMENTATION**

Throughout implementation, reference these key CAI files:

**Architecture & Patterns:**
- `cai/docs/cai_architecture.md` - 8 pillars, formal pattern definition
- `cai/docs/agents.md` - Agent structure, capabilities, best practices
- `cai/docs/handoffs.md` - Handoff protocol and examples
- `cai/docs/guardrails.md` - Guardrail implementation

**Code References:**
- `cai/src/cai/agents/factory.py` - Agent factory pattern
- `cai/src/cai/agents/web_pentester.py` - Security agent example
- `cai/src/cai/agents/patterns/` - Pattern implementations
- `cai/src/cai/prompts/` - System prompts for all agents

**Integration:**
- `cai/docs/mcp.md` - MCP integration
- `cai/examples/agent_patterns/` - Pattern usage examples

---

## **COMPLETION CHECKLIST**

Mark complete when ALL of these exist and work:

**Files Created (Minimum 30 files):**
- [ ] 15 core system files (model discovery, registry, orchestrator, workflows)
- [ ] 8 pattern/handoff files (patterns, handoffs, guardrails, messaging)
- [ ] 6 memory files (vector, structured, task memory)
- [ ] 5 agent files (web_pentester, retester, report, factory, model)
- [ ] 4 tool files (security_tools, builtin_tools, tool_registry, dynamic_tools)
- [ ] 3 monitoring files (telemetry, human_checkpoint, dashboard)
- [ ] 3 example workflows (security_assessment, code_generation, dynamic_models)

**Capabilities Verified:**
- [ ] User enters goal â†’ Orchestrator decomposes â†’ Agents execute â†’ Human approves â†’ Output delivered
- [ ] New model installed â†’ Auto-detected â†’ Capabilities assessed â†’ Available for selection
- [ ] Security workflow: web_pentester â†’ retester â†’ report with handoffs
- [ ] Dangerous command blocked by guardrails
- [ ] Code executed safely in Docker sandbox
- [ ] Vector search retrieves relevant past outputs
- [ ] CLI dashboard shows real-time progress

---

## **START COMMAND FOR QUEST MODE**

```
Implement the complete Multi-Agent Orchestration System with CAI Framework Integration as specified in this prompt. Execute all 9 phases in order (P0 â†’ P1 â†’ P2), creating all required files with complete, production-ready code. After each phase, validate functionality before proceeding. Reference the CAI repository files as specified for each component. Ensure all critical success criteria are met before marking complete. Begin with Phase 1: Core Infrastructure.
```

---

This prompt is **comprehensive, structured, and actionable** for Quest Mode. It includes:
âœ… **All CAI integrations** we analyzed today  
âœ… **Complete file structure** with 30+ files  
âœ… **Concrete implementation requirements** for every component  
âœ… **CAI reference files** for each task  
âœ… **Validation criteria** after each phase  
âœ… **Priority ordering** (P0 â†’ P1 â†’ P2)  
âœ… **Final integration test** workflow  

Quest Mode can now systematically implement EVERYTHING! ðŸš€